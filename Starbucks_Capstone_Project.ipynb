{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Project\n",
    "__Author: Do Lee__\n",
    "__Github: github.com/do-y-lee/udacity-starbucks-capstone-project__\n",
    "__LinkedIn: linkedin.com/in/dolee/__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "__Note:__ In this project, an offer is represented by four attributes concatenated together, as shown below. As a result, a unique offer is represented by type, reward, difficulty, and duration. In the train and test dataset, this offer representation is found under offer_type_v2 column. Throughout the project, when it refers to offer or offer_type, it is referring to this representation.\n",
    "\n",
    "\n",
    "| Offer/Offer Type: type-reward-difficulty-duration |\n",
    "| :-- |\n",
    "| bogo-5-5-168 |\n",
    "| bogo-10-10-168 |\n",
    "| bogo-5-5-120 |\n",
    "| bogo-10-10-120 |\n",
    "| discount-5-20-240 |\n",
    "| discount-2-10-240 |\n",
    "| discount-2-10-168 |\n",
    "| discount-3-7-168 |\n",
    "\n",
    "\n",
    "The goal of the project is to predict which offer has the highest probability of completion for each customer. There are eight offer types, four BOGO and four discount offers, excluding informational ones. In order to predict at this granular level, eight separate classification models are developed to produce eight predictions for each customer based on aggregated lifetime features. \n",
    "\n",
    "For a batch of targeted customers, eight predictions are generated for every customer. Among the positive predictions, the offer with the highest prediction probability is selected as the winner. If there is a tie, the first winning offer in the array is selected. If there are only negative predictions, the customer will receive a randomly selected offer from top four performing offers. \n",
    "\n",
    "***\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "#### Goal: \n",
    "* Starbucks is using offers to __drive higher customer engagement and spends__. The data shows that average transaction size with completed offers is higher than transactions without completed offers. The goal is to __maximize the potential of offers__ by creating predictive models to target customers with offers having the highest probability of success. More specifically, eight supervised classification models are developed each representing a specific offer type. Two informational offers are excluded because there are nothing to redeem or complete. Informational offers with social channel has a high view rate (approximately 90%) and thus this can be used to send messages to customers separately from BOGO and discount offers.\n",
    "\n",
    "#### Opportunity:\n",
    "* The opportunity can expressed by a single data point - only 22% of all transactions have completed offers. Furthermore, 77% of customers have at least one completed offer and most customers have received offers. Therefore, the opportunity is to increase 22% to something much higher so that more transactions have completed offers leading to higher spends and higher engagement. \n",
    "* Although less than a quarter of the transactions have a completed offer or completed offers, data shows that completed offers generate higher average transaction size. At the same time, money is given away through BOGO and discount offers to entice customers to spend. Over the long run, the strategy contributes to a larger lifetime value of the customer and more than make up for the rewards given to customers.\n",
    "* Therefore, with the right balance and identifying offers with higher probability of conversion per customer, we can achieve a higher ROI. Moreover, this will result in longer customer retention and higher customer lifetime value.\n",
    "\n",
    "#### Approach:\n",
    "* First, to make analyzing and aggregating data faster and less repetitive, two data marts are created. \n",
    "    1. Offer funnel view: This view flattens the transcript data using the offer received events as the base. In terms of SQL, the offer received events would be the left table and all other events, such as viewed and completed offer events, would be joined to the left table where the matches are made using join keys. \n",
    "        * `datamart_offer_funnel_view.py`\n",
    "    2. Transaction engagement view: This dataset is the most important due to the fact that the transaction events are used as the base. All offer events are attributed and mapped back to the transactions where completed offers are attached. This allows fast way to understand which offers received are being viewed and completed.\n",
    "        * `datamart_transaction_engagement.py`\n",
    "* Next, the customer_id's are used to split the data into train (67%) and test (33%) sets. Using the train set, two feature engineering/transformation layers are created. The first layer creates aggregated/lifetime metrics at the customer level. The second layer further transforms the features so that they can be used as input features for random forest classification models. All features are listed below.\n",
    "    * `preprocessor_feat_engine_layer_1.py`\n",
    "    * `preprocessor_feat_engine_layer_2.py`\n",
    "\n",
    "***\n",
    "\n",
    "| Feature Name | Feature Type | Data Type | Description |\n",
    "| :- | :- | :- | :- |\n",
    "| customer_id | uuid | string | Unique identifier per customer |\n",
    "| gender | binary | string | Describes the sex of the customer | \n",
    "| age | continuous | integer | Numeric age of the customer | \n",
    "| age_quantile_range | interval | string | 5 age quantile ranges (e.g., [\\\\$0-\\\\$20k]) |\n",
    "| age_quantile_label | interval | string | 5 age quantile labels (e.g., 0-20Q) |\n",
    "| date_registered | date | date | Date when customer became a Starbucks member with format YYYY-MM-DD |\n",
    "| days_registered | continuous | integer | Represents how long a customer has been a member in days from the max available registered date in the train dataset |\n",
    "| days_reg_quantile_range | interval | string | 10 days registered quantile ranges |\n",
    "| days_reg_quantile_label | interval | string | 10 days registered quantile labels |\n",
    "| income | continuous | float | Numeric value representing the customer's annual income |\n",
    "| income_quantile_range | interval | string | 5 income quantile ranges |\n",
    "| income_quantile_label | interval | string | 5 income quantile labels |\n",
    "| transaction_amount | continuous | float | Represents total summed amount of customer's transactions |\n",
    "| transaction_cnt | continuous | float | Represents total number of transactions per customer |\n",
    "| transaction_aos | continuous | float | Represents the average transaction size (transaction_amount / transaction_cnt) per customer |\n",
    "| gender_NA | binary | integer | Binary flag indicates customer's gender is missing |\n",
    "| income_NA | binary | integer | Binary flag indicates customer's income is missing |\n",
    "| age_NA | binary | integer | Binary flag indicates customer's age is missing |\n",
    "| num_offer_received | continuous | integer | Total number of offers received by a customer |\n",
    "| num_bogo_offer_received | continuous | integer | Total number of BOGO offers received by a customer |\n",
    "| num_info_offer_received | continuous | integer | Total number of informational offers received by a customer |\n",
    "| num_discount_offer_received | continuous | integer | Total number of discount offers received by a customer |\n",
    "| num_offer_viewed | continuous | integer | Total number of offers viewed by a customer |\n",
    "| num_offer_completed | continuous | integer | Total number of offers completed by a customer |\n",
    "| num_offer_completed_viewed | continuous | integer | Total number of offers completed where offers were viewed by the customer\n",
    "| num_offer_completed_not_viewed | continuous | integer | Total number of offers completed where offers were not viewed by the customer |\n",
    "| num_transactions_no_oc | continuous | integer | Total number of transactions without any completed offers per customer |\n",
    "| num_transactions_oc_direct | continuous | integer | Number of transactions with completed offers where the transaction amount was equal or greater than the offer difficulty treshold amount | \n",
    "| percent_oc_direct_transactions | continuous | float | Percentage of transactions with completed offers where the transaction amount was equal or greather than the offer difficulty threshold amount |\n",
    "| num_transactions_oc_indirect | continuous | integer | Number of transactions with completed offers where the transaction amount was less than the offer difficulty threshold amount, which is an indication of cummulated spends meeting the difficulty threshold over time |\n",
    "| avg_offered_received_freq | continuous | float | On average, how often does the customer receive an offer (in hours) |\n",
    "| info_view_rate | continuous | float | Number of informational offers viewed divided by number of informational offers received |\n",
    "| offer_view_rate | continuous | float | Number of offers viewed divided by number of offers received | \n",
    "| offer_completion_rate | continuous | float | Number of offers completed divided by number of offers received minus number of informational offers received |\n",
    "| total_reward_amount | continuous | float | Total reward amount the customer has acquired by completing offers |\n",
    "| avg_reward_per_oc_transaction | continuous | float | Among all transactions with completed offers, calculate the average reward acquired per transaction |\n",
    "| transaction_oc_amount | continuous | float | Total transaction amount with completed offers per customer |\n",
    "| transaction_aos_oc | continuous | float | Average order size with completed offer transactions | \n",
    "| transaction_no_oc_amount | continuous | float | Total transaction amount with transactions without completed offers |\n",
    "| transaction_aos_no_oc | continuous | float | Average order size of transactions without completed offers |\n",
    "| num_bogo_offer_viewed | continuous | integer | Number of BOGO offers viewed per customer |\n",
    "| num_info_offer_viewed | continuous | integer | Number of informational offers viewed per customer |\n",
    "| num_discount_offer_viewed | continuous | integer | Number of discount offers viewed per customer |\n",
    "| num_bogo_offer_completed | continuous | integer | Number of BOGO offers completed per customer |\n",
    "| num_discount_offer_completed | continuous | integer | Number of discount offers completed per customer |\n",
    "| median_offer_duration | continuous | float | Calculate the median offer duration based on all the offers received per customer |\n",
    "| avg_offer_completion_time | continuous | float | sum(offer_completed_time - offer_received_time) / count(offer_completed); Unit is in hours |\n",
    "| avg_hrs_bw_transactions | continuous | float | Calculate the average time in hours between transactions if the customer has more than one transaction |\n",
    "| num_oc_ch_web | continuous | integer | Number of completed offers where it was advertised through the web channel |\n",
    "| num_oc_ch_social | continuous | integer | Number of completed offers where it was advertised through the social channel |\n",
    "| num_oc_ch_mobile | continuous | integer | Number of completed offers where it was advertised through the mobile channel |\n",
    "| num_oc_ch_email | continuous | integer | Number of completed offers where it was advertised through the email channel |\n",
    "\n",
    "***\n",
    "\n",
    "* At this stage, the train set is split by offer_type (v2), which creates eight train datasets. Before the final models are created, Pearson correlation, random forest feature importance, and permutation importance techniques are used to select most important features to train the final models. At the end of this process, a list of important feature names are saved for each offer type and saved as JSON file.\n",
    "    * `preprocessor_feat_select_pipeline.py`\n",
    "\n",
    "    * `trained_models/training_feature_sets.json`\n",
    "```json\n",
    "{\n",
    "  \"bogo-5-5-168\": [\n",
    "    \"num_bogo_offer_completed\",\n",
    "    \"total_reward_amount\",\n",
    "    \"offer_completion_rate\",\n",
    "    \"offer_received_time\",\n",
    "    \"avg_reward_per_oc_transaction\",\n",
    "    \"median_offer_duration\",\n",
    "    \"num_transactions_oc_direct\",\n",
    "    \"num_offer_completed\",\n",
    "    \"percent_oc_direct_transactions\",\n",
    "    \"transaction_aos_oc\",\n",
    "    \"num_oc_ch_social\",\n",
    "    \"avg_offer_completion_time\",\n",
    "    \"transaction_aos_no_oc\",\n",
    "    \"transaction_no_oc_amount\",\n",
    "    \"num_offer_completed_not_viewed\",\n",
    "    \"transaction_amount\",\n",
    "    \"num_bogo_offer_received\",\n",
    "    \"transaction_cnt\"\n",
    "  ], ...\n",
    "}\n",
    "```\n",
    "\n",
    "* Using the train v3 dataset coupled with important features identified during feature selection, eight random forest classification models are created and pickled (serialized) so that it can be used later to make predictions. The test v3 dataset is used to make predictions for each offer type, and the results are saved.\n",
    "    * `model_train_predict_pipeline.py`\n",
    "    \n",
    "* Once the test prediction files are saved with prediction probabilities, the recommendation logic will determine which offer will be the winner for each customer. For each customer, eight predictions are generated. Among all predictions, only the positive predictions are isolated. Next, the one with the highest prediction probability is chosen as the winner. If the customer has only negative predictions, a random offer will be chosen among top four performing offers.\n",
    "    * `model_make_recommedation_pipeline.py`\n",
    "\n",
    "#### Full Pipeline Design:\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/offer_model_pred_flow.png\" style=\"width:850px;height:700px\"/>\n",
    "\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Background:\n",
    "In building a binary classification model, there is a cost associated with certain misclassifications. For example, if you are making a diagnosis if a patient has a disease or not, then you want to minimize false negative results. This means that the patient has the disease, but the diagnosis comes back negative. In this case, it's better to receive a false positive result than a false negative one due to the impact on human life. Therefore, the cost of false negatives are much higher. As a result, building a cost-sensitive model and finding the best tradeoff in making predictions become essential.\n",
    "\n",
    "The same line of thinking is applicable to Starbucks challenge, although not as serious as disease diagnosis. The goal is to make customer use more offers. It will be helpful to think through the cost associated with misclassifications, false positives and false negatives, which in turn helps put more weight on certain model diagnostic metrics.\n",
    "\n",
    "Therefore, we want to avoid false postives and false negatives. However, because we are aiming to convert as many customers as possible, false negatives would be relatively better than false positives. Model said a customer would not convert, but converted. This would be overall bad for model performance and even worse if these come up more frequently, but from a marketing perspective it's another offer completion and thus a win. For model performance, what we want here is minimize both false positives and false negatives, and maximize true positives.     \n",
    "#### Model Diagnostic Metrics\n",
    "* __Precision:__ This will be one of the leading metrics used to measure the performance of random forest classifiers. Precision is important because it measures how well the model successfully predicts true positives among all predicted positives [TP/(TP + FP)]. For this project, we want to maximize precision. In doing so, this helps minimize false positives.  \n",
    "\n",
    "* __Recall:__ Recall is another leading metric we want to maximize. Recall is important because it tells us how many actual positives (true positives and false negatives) were predicted correctly in train and test. In the background section, I talked about minimizing false negatives and thus maximizing recall. This is where recall comes into the performance picture [TP/(TP + FN)]. Recall is also known as true positve rate and sensitivity. \n",
    "\n",
    "* __F1 Score:__ This will be the __main metric__ used to measure model performance success. This metric is a harmonic mean calculated using precision and recall [(2 * precision * recall)/(precision + recall)]. Therefore, maximizing the F1 score maximizes precision and recall as well, and essentially finds the best balance between the two metrics. This is known as the precision-recall tradeoff and this can be represented visually by the precision-recall curve, which I'll produce later on. As precision increases, recall decreases, and vice versa.  \n",
    "    * __Decision Threshold:__ We can further try to maximize the F1 score by tuning the decision threshold, which often is set at 0.5. By moving the decision threshold up or down, we can further improve the F1 score. This method can optimize either precision or recall, but in this case we are optimizing for both using the F1 score. I'll share my analysis later on.\n",
    "\n",
    "* __ROC AUC:__ This is another leading metric and will provide valuable insights into model performance. AUC is the area under the ROC curve and the value usually has a range between 0.5 and 1.0. The ROC curve plots false positive rate on the x-axis and the true positive rate (recall) on the y-axis. This can help locate the best operating point on the curve. For this project, we want to drive the AUC as closest to 1.0 as possible. This means that we are maximizing true positive rate while minimizing false positive rate [FP/(FP + TN)]. False positive rate also can be calculated by (1 - specificity), where specificity is true negative rate [TN/(TN + FP)] and captures the proportion of true negatives among all actual negatives. \n",
    "\n",
    "* __Accuracy:__ Accuracy will be used as a secondary metric and a metric to gauge performance at a high level. This measures how well model performs overall. Accuracy collects all correct positves and negatives and divide by total predictions [(TP + TN)/(TP + TN + FP + FN)].\n",
    "\n",
    "#### Marketing Campaign Success Metrics\n",
    "* __Offer Completion Rate, Offer View Rate, and Average Spends per Customer:__ From a machine learning perspective, the models might not be performing well; however, if the completion rate is higher than before, this could potential be a win for marketing. The models are getting some of the predictions right and completion rate might tick up. Same logic applies to view rate and average spends per customer. If the opposite is true, then we have a big problem and back to the drawing boards.\n",
    "* __What about the customers who received negative predictions?__ From a marketing standpoint, you want to maximize success and even if some customers are predicted to reject offers you want to target them with something in order to drive incremental gain. One strategy could be to let the model predict away and see if the predicted negatives are truly negatives. Over time, the team could create a heuristic approach to target these customers and gradually convert them if possible. For this project, I am randomly selecting one of the four top performing offers and targeting customers who received negative predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "* __Data Marts:__ I extensively explored the raw data, which includes transcript.json, profile.json, and portfolio.json files. In order to expedite my exploratory analysis, I created two views, an offer funnel view (offer data mart) and a transaction engagement view (transaction data mart). I'm calling these data marts because they allow quick aggregations of offers and transactions. I also introduce new identifiers to properly, for example, define transaction groupings (row_id's) when multiple offers were attached to a single transaction. There are total of four identifiers, row_id, transaction_id, customer_id, and offer_id. For the transaction data mart, I attributed all received offers, viewed offers, and completed offers to the appropriate transaction_id. I built a custom attribution logic to attach viewed offers and received offers. The attribution logic to attach the correct received offers work about 98% of the time. I noticed a slight issue, but for this exercise this will be good enough.  \n",
    "    * `datamart_offers_funnel_view.py`\n",
    "    * `datamart_transaction_engagement.py`\n",
    "\n",
    "* __Feature Transformation & Engineering:__ I introduce two layers of feature transformation and engineering. I also created a class to quickly output base dataframes. Using the three provided raw JSON files, rows with missing values and outliers were replaced with corresponding median values. The entire list of features can be found under \"Problem Statement - Approach\". For more details, refer to these files.\n",
    "    * `base_transforms/base_transforms_df.py`\n",
    "    * `preprocessor_feat_engine_layer_1.py`\n",
    "    * `preprocessor_feat_engine_layer_2.py`\n",
    "   \n",
    "* __Insights:__\n",
    "    * Number of customers who have completed offers is relatively high, where 77% of all transacted customers have used offers.\n",
    "    * 97.5% of customers in the profile data have at least one transaction regardless of completing an offer or not. Overall, conversion isn't the problem. The challenge is using offers to amplify every aspect of the business. Does it increase spend? Does it increase retention?\n",
    "    * Although 77% of customers have at least one offer completed, the percentage of transactions with offer completed is only 22%.\n",
    "    * Average transaction size with offer completed is overall higher than transactions with no offer completed.\n",
    "    \n",
    "    ![t-test](output/diagnostic_metrics/sample-t-test.png) \n",
    "    \n",
    "    * When closely examining the offer funnel, it becomes clear that all offers with social channel perform the best. Thus, leveraging social media becomes an important component in increasing view rate and completion rate.\n",
    "    \n",
    "    ![offer_funnel_analysis](output/diagnostic_metrics/offer_funnel_analysis.png) \n",
    "\n",
    "### Exploratory Visualization\n",
    "* __Time series chart__ can be powerful in showing big trends over time. This chart shows offer received, viewed, and completed events as well as number of transaction events over time (in hours). We can clearly see the peaks and troughs. For each offer send date, we see an immediate jump in all events. We also see a gradual increase in transactions, and a loose correlation between offer sends and transactions can be observed. This could be a weekly spend cycle, but we also observe increase in transaction events in some offer send dates. \n",
    "\n",
    "![time_series](output/diagnostic_metrics/offers_time_series.png) \n",
    "\n",
    "* __Density plots__ of average transaction size with or without offer completed can show us if offers are making a difference in driving higher spends and helping topline (gross revenue). We do see this happening in the plots. \n",
    "    * transaction_aos: overall transaction average order size\n",
    "    * transaction_aos_no_oc: transaction average order size without offer completed\n",
    "    * transaction_aos_oc: transaction average order size with offer completed\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/transaction_size_density_plot.png\" style=\"width:900px;height:500px\"/>\n",
    "\n",
    "* __Feature histograms__ help quickly understand variable distribution in training data and become an input in providing insights into how we would go about transforming the features as well as engineering new ones.\n",
    "\n",
    "![feature_histograms1](output/diagnostic_metrics/train_feature_histograms_1.png)\n",
    "![feature_histograms2](output/diagnostic_metrics/train_feature_histograms_2.png)\n",
    "\n",
    "\n",
    "### Algorithms and Techniques\n",
    "* __Algorithm:__ Random forest classifier was chosen to predict offer conversion. I decided on this classifier due to its flexibility and robustness. It is an ensemble model and generally produces solid predictions. It uses impurity-based approach to feature importance. As a result, random forest classifier can be used for both feature selection and building a binary classification model.\n",
    "    * __Hyperparameter tuning:__ Random forest classifier also has its set of hyperparameters, usually related to leaf and node splits. These can be tuned using randomized search or grid search to further improve the model. For the model tuning step, I experimented with randomized search to tune hyperparameters. The tuning process did not significantly improve the model. Therefore, tuning the hyperparameters won't be necessary for ths project.\n",
    "    * __Decision threshold tuning:__ Another opportunity to further tune the model is determining the best decision treshold, which is usually set at 0.5. Based on which misclassifications to minimize, the decision threshold can be manipulated to locate the new optimal point and potentially improve the model. I went ahead and conducted the decision threshold analysis, which will be shared later. The model threholds were already pretty close to the optimal threholds and decided not to make changes.\n",
    "\n",
    "* __Feature Selection:__ I applied what felt right for this process. First, I wanted to remove correlated features. Second, I wanted to use both random forest feature importance and permutation importance to determine important features for each offer type. I created an arbitrary threshold for each importance method, and overlapping features made up the final feature set. After correlated features were removed, I trained eight random forest models using eight different training sets (each representing offer type) and extracted the important features. I ran each training set through permutation importance function as well. Next, I identified overlapping features. These feature sets were serialized as JSON to be used later when creating model estimators.\n",
    "\n",
    "* __Model Estimators:__ Leveraging the serialized JSON feature sets and training v3 data, I created eight estimators and pickled each to be used later for making test predictions. There is not much class imbalance in the datasets so hyperparamter and decision threshold tuning steps were not absolutely necessary. However, I went through the exercise of diving deeper into randomized search to tune hyperparameters and determining the optimal threshold for each estimator, as mentioned above. The results did not incrementally improve the models in a significant way.\n",
    "\n",
    "\n",
    "### Benchmark\n",
    "* __Zero Rule Algorithm Classification:__ ROC_AUC and accuracy provide a consistent picture of no-skill model per offer type. The zero rule algo picks the majority class and generates the confusion matrix, which in turn creates the diagnostic metrics. In some cases, the positive is the majority class while in some cases the negative is the majority class, which is reflected in the baseline precision and recall values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# zero rule algorithm for classification\n",
    "def zeror_algo_clf(offer_type, train, test):\n",
    "    outputs = train[train.offer_type_v2==offer_type].offer_completed.tolist()\n",
    "    prediction = max(set(outputs), key=outputs.count)\n",
    "    n_test = test[test.offer_type_v2==offer_type].shape[0]\n",
    "    predicted = [prediction for _ in range(n_test)]\n",
    "    return predicted\n",
    "\n",
    "def naive_baseline_model(train, test):\n",
    "    grouped_metrics = {}\n",
    "    offer_types = train.offer_type_v2.unique()\n",
    "    for offer_type in offer_types:\n",
    "        offer_type_metrics = []\n",
    "        y_baseline_pred = zeror_algo_clf(offer_type, train, test)\n",
    "        y_actual_test = test[test.offer_type_v2==offer_type].offer_completed.to_list()\n",
    "        tn, fp, fn, tp = confusion_matrix(y_actual_test, y_baseline_pred).ravel()\n",
    "        precision = round((1.0 * tp)/(tp + fp), 4)\n",
    "        recall = round((1.0 * tp)/(tp + fn), 4)\n",
    "        f1_score = round((2 * precision * recall)/(precision + recall), 4)\n",
    "        roc_auc = roc_auc_score(y_actual_test, y_baseline_pred)\n",
    "        accuracy_score = round(1.0 * (tp + tn) / (tn + fp + fn + tp), 4)\n",
    "        offer_type_metrics.append(precision)\n",
    "        offer_type_metrics.append(recall)\n",
    "        offer_type_metrics.append(f1_score)\n",
    "        offer_type_metrics.append(roc_auc)\n",
    "        offer_type_metrics.append(accuracy_score)\n",
    "        grouped_metrics[offer_type] = offer_type_metrics\n",
    "    column_names = ['baseline_precision', 'baseline_recall', 'baseline_f1_score', \n",
    "                    'baseline_roc_auc', 'baseline_accuracy']\n",
    "    df = pd.DataFrame.from_dict(grouped_metrics, orient = 'index', \n",
    "                                columns = column_names).reset_index().rename(columns={'index': 'offer_type'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>baseline_precision</th>\n",
       "      <th>baseline_recall</th>\n",
       "      <th>baseline_f1_score</th>\n",
       "      <th>baseline_roc_auc</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>0.5642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>0.5014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6679</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>0.6517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7891</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  baseline_precision  baseline_recall  baseline_f1_score  \\\n",
       "0       bogo-5-5-168              0.5518              1.0             0.7112   \n",
       "1     bogo-10-10-168              0.0000              0.0             0.0000   \n",
       "2       bogo-5-5-120              0.5642              1.0             0.7214   \n",
       "3  discount-5-20-240              0.0000              0.0             0.0000   \n",
       "4  discount-2-10-240              0.6839              1.0             0.8123   \n",
       "5     bogo-10-10-120              0.0000              0.0             0.0000   \n",
       "6  discount-2-10-168              0.5014              1.0             0.6679   \n",
       "7   discount-3-7-168              0.6517              1.0             0.7891   \n",
       "\n",
       "   baseline_roc_auc  baseline_accuracy  \n",
       "0               0.5             0.5518  \n",
       "1               0.5             0.5058  \n",
       "2               0.5             0.5642  \n",
       "3               0.5             0.5678  \n",
       "4               0.5             0.6839  \n",
       "5               0.5             0.5676  \n",
       "6               0.5             0.5014  \n",
       "7               0.5             0.6517  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute naive_baseline_model function\n",
    "X_train_v3 = pd.read_csv('data/train_v3_starbucks.csv.gz', compression = 'gzip')\n",
    "X_train_v3 = X_train_v3[X_train_v3.offer_type != 'informational']\n",
    "X_test_v3 = pd.read_csv('data/test_v3_starbucks.csv.gz', compression = 'gzip')\n",
    "X_test_v3 = X_test_v3[X_test_v3.offer_type != 'informational']\n",
    "df_baseline = naive_baseline_model(X_train_v3, X_test_v3).fillna(0)\n",
    "df_baseline.to_csv('output/diagnostic_metrics/baseline_metrics.csv.gz', index = False, compression = 'gzip')\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "* __Transaction Engagement:__ `datamart_transaction_engagement.py`\n",
    "    * Attribution of all offer events to corresponding transactions.\n",
    "    * This view allows what offers are being completed with transactions as well as which offers have been receive and viewed at the transaction level.\n",
    "\n",
    "* __Feature Engineering Layer 1:__ `precessor_feat_engine_layer_1.py`\n",
    "    * __missing_outlier_imputer__ - Imputes missing values and outliers with corresponding median values. Rows were not dropped due to small number of occurrences. Age outliers were imputed with median age and any data points that looked like outliers were kept because these data points can occur in the real world.\n",
    "    * __age_quantile_transformer__ - Placing customer's age into one of five quantile bins.\n",
    "    * __income_quantile_transformer__ - Placing customer's income into one of five quantile bins.\n",
    "    * __date_registered_transformer__ - Member date is changed to days (available max date - member's date) and each customer's member date is placed in one of ten quantile bins.\n",
    "    * __offer_received_transformer__ - For each customer, create number of offers received for each offer name (BOGO, discount, and informational).\n",
    "    * __offer_viewed_transformer__ - For each customer, create number of offers viewed for each offer name (BOGO, discount, and informational).\n",
    "    * __offer_completed_transformer__ - For each customer, aggregate total number of offers completed, number of offers completed that were viewed, and number of offers completed that were not viewed.\n",
    "    * __offer_completed_by_offer_type__ - For each customer, aggregate number of offers completed by offer name (BOGO, discount, and informational).\n",
    "    * __transaction_count_transformer__ - \n",
    "        * Create num_transactions_no_oc (no completed offer attached), \n",
    "        * num_transactions_oc_direct (number of transactions where offer threshold was met in a single transaction), \n",
    "        * num_transactions_oc_indirect (number of transactions where offer threshold was met over time), and\n",
    "        * percent_oc_direct_transactions (percent of transactions where offers were completed in the single transaction).\n",
    "    * __offer_ratio_calculations__ - Calculate offer_view_rate, offer_completion_rate, and info_view_rate.\n",
    "    * __offer_reward_transformer__ - Calculate total_reward_amount and avg_reward_per_oc_transaction (average reward for transaction with offer completed).\n",
    "    * __transaction_amount_transformer__ - For each customer, create transaction_oc_amount (transaction amount with offers completed), transaction_aos_oc (average transaction size with offers completed), transaction_no_oc_amount (transaction amount without offers completed), and transaction_aos_no_oc (average transaction size without any offers completed).\n",
    "    * __median_offer_duration_calc__ - For all the offers received per customer, calculate the median duration of all offers received.\n",
    "    * __avg_offer_completion_time_calc__ - Calculate avg_offer_completion_time for each customer if possible. sum(offer_completed_time - offer_received_time) / count(offer_completed)\n",
    "    * __avg_hrs_bw_trans__ - Function iterates over each customer_id and creates a list of all transaction_time per transaction_id, and calculates the difference between current and previous transaction, and then averages the spreads.\n",
    "    * __offer_channel_counter__ - For each customer, create num_oc_ch_social, num_oc_ch_web, num_oc_ch_mobile, and num_oc_ch_email. Count of offer completed attached to channels.\n",
    "    * __avg_offer_received_frequency__ - Calculates the average offer received frequency. Basically, taking all the offers received per customer, and taking the difference from current to previous until the very first one. Once you have time differences, you take the average to get an understand how often the customers received offers.\n",
    "   \n",
    "* __Feature Engineering Layer 2:__ `precessor_feat_engine_layer_2.py`\n",
    "    * __drop_constant_features__ - Identify constant features and drop them if any. `config.py`\n",
    "    * __drop_generic_features__ - Drop manually identified generic features. `config.py`\n",
    "    * __gender_one_hot_encoder__ - Gender converted with one hot encoding.\n",
    "    * __ordinal_feature_encoder__ - 'age_quantile_label', 'days_reg_quantile_label', and 'income_quantile_label' features converted to ordinal features. `config.py`\n",
    "    * __drop_redundant_features__ - Drop manually identified redundant features. `config.py`\n",
    "    * __create_train_test_v1__ - Staging step for train and test set.\n",
    "    * __create_portfolio_expanded__ - Staging step.\n",
    "    * __create_received_expanded__ - Staging step.\n",
    "    * __create_train_test_v2__ - Staging step.\n",
    "    * __create_train_test_v3__ - Final train and test set.\n",
    "    \n",
    "* __Feature Selection:__ `precessor_feat_select_pipeline.py`\n",
    "    * __feature_importance_ranking__ - Random forest feature importance ranking.\n",
    "    * __permutation_importance_ranking__ - Scikit-learn permutation importance ranking for features.\n",
    "    * __feature_selection_per_offer_type__ - Full feature selection function with removing correlated features and implementing two ranking functions from above.\n",
    "    * __serialize_feature_sets__ - Serializes the feature selection function output for each offer type for later use.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "* __Overview:__ Once all the scripts are created, the implementation is straightforward. In the design flow diagram below, the key step is the \"Model: Training & Prediction Pipeline\". In this step, the train v3 set is split into eight training datasets for each offer type. Each training set is used to train a random forest model and the serialized feature set JSON is used as input to map important features to each training data. Furthermore, the pickled models are used to make predictions on the testing set. Finally, the recommendation logic assigns a winning offer per customer.\n",
    "    * `preprocessor_feat_engine_layer_1.py`\n",
    "        * data/train_starbucks.csv.gz (train v1)\n",
    "        * data/test_starbucks.csv.gz (test v1)\n",
    "    * `precessor_feat_engine_layer_2.py`\n",
    "        * data/train_v3_starbucks.csv.gz\n",
    "        * data/test_v3_starbucks.csv.gz\n",
    "    * `precessor_feat_select_pipeline.py`\n",
    "        * trained_models/training_feature_sets.json\n",
    "    * `model_train_predict_pipeline.py`\n",
    "        * trained_models/\n",
    "            * bogo-5-5-120_model.pickle\n",
    "            * bogo-5-5-168_model.pickle\n",
    "            * bogo-10-10-120_model.pickle\n",
    "            * bogo-10-10-168_model.pickle\n",
    "            * discount-2-10-168_model.pickle\n",
    "            * discount-2-10-240_model.pickle\n",
    "            * discount-3-7-168_model.pickle\n",
    "            * discount-5-20-240_model.pickle\n",
    "        * output/predictions/\n",
    "            * bogo-5-5-120_test_predictions.csv.gz\n",
    "            * bogo-5-5-168_test_predictions.csv.gz\n",
    "            * bogo-10-10-120_test_predictions.csv.gz\n",
    "            * bogo-10-10-168_test_predictions.csv.gz\n",
    "            * discount-2-10-168_test_predictions.csv.gz\n",
    "            * discount-2-10-240_test_predictions.csv.gz\n",
    "            * discount-3-7-168_test_predictions.csv.gz\n",
    "            * discount-5-20-240_test_predictions.csv.gz\n",
    "    * `model_make_recommendation_pipeline.py`\n",
    "        * output/recommendations/\n",
    "            * model_recommendations.csv.gz\n",
    "            * random_recommendations.csv.gz\n",
    "    * `output_model_stats.py`\n",
    "        * output/diagnostic_metrics/\n",
    "            * train_model_stats.csv.gz\n",
    "            * test_model_stats.csv.gz\n",
    "            * test_optimal_f1_thresholds.csv.gz\n",
    "            * test_roc_curves.png\n",
    "            * test_pr_curves.png\n",
    "***\n",
    "* __Train & Predict Deep Dive:__ The outcome at this stage is eight pickled random forest estimators and eight test prediction CSV files. Using the pickled models and saved test predictions, I can easily generate diagnostic metrics and apply the recommendation logic to produce offer recommendations.\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/starbucks_implementation.png\" style=\"width:600px;height:700px\"/>\n",
    "\n",
    "* __Recommendation Logic Deep Dive:__ Using the testing set, we make predictions with the assumption these are the customers we will be targeting in the next offer batch send. Essentially, each customer with corresponding features is plugged into eight models to generate positive or negative predictions. Among eight predictions per customer, the positive ones are isolated. Among the positive predictions, the maximum prediction probability is determined and the corresponding offer type is the winner. If customer has only negative predictions, the customer will receive a randomly chosen offer type from the top four performing offers.\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/starbucks_recommendation_flow.png\"/>\n",
    "\n",
    "***\n",
    "\n",
    "### Refinement\n",
    "* __Randomized Search Hyperparameter Tuning:__ For each random forest model, I used randomized search to tune model hyperparameters. Based on how I set up the randomized search, the run time was about 10 minutes per offer type model. I assessed the model improvement and the gains were not huge for the amount of effort. As a result, I made the decision of using the default hyperparameters.\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/offers_randomized_search.png\" style=\"width:700px;height:400px\"/>\n",
    "\n",
    "* __Decision Threshold Optimization:__ `output_model_stats.py`\n",
    "    * Because one of the main success metrics is the F1 score, I wanted to see if the decision threshold could be further tuned and optimized by improving the F1 score. I took a closer look at precision and recall by creating the precision-recall curves using the testing set. \n",
    "    * Can we move the decision threshold to make better predictions on the test set? In order to achieve this, what we want to do is maximize the F1 score with the understanding that there is a tradeoff between precision and recall. By locating the maximum F1 score, we are locating the maximum operating point on the precision-recall curve and the decision threshold associated with this point can be mapped. This logic is captured under the `output_model_stats.create_and_save_precision_recall_curve` function.\n",
    "    \n",
    "    <img src=\"output/diagnostic_metrics/test_pr_curves.png\" style=\"width:600px;height:600px\"/>\n",
    "    \n",
    "    * The output of the function shows the best decision threshold for each model. When making predictions with new data or test data, these new decision thresholds can be used to determine positive or negative outcome per customer per model. My decision was not to modify the thresholds for future predictions because the difference between default and modified thresholds were not too different. The gains were small, but any other future improvements could incorporate these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>optimal_decision_threshold</th>\n",
       "      <th>optimal_f1_score</th>\n",
       "      <th>optimal_precision</th>\n",
       "      <th>optimal_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.941058</td>\n",
       "      <td>0.896046</td>\n",
       "      <td>0.990832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.951044</td>\n",
       "      <td>0.917949</td>\n",
       "      <td>0.986614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.940105</td>\n",
       "      <td>0.901882</td>\n",
       "      <td>0.981712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.943446</td>\n",
       "      <td>0.906487</td>\n",
       "      <td>0.983547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.964458</td>\n",
       "      <td>0.937017</td>\n",
       "      <td>0.993556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.935721</td>\n",
       "      <td>0.931444</td>\n",
       "      <td>0.940037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.926541</td>\n",
       "      <td>0.887553</td>\n",
       "      <td>0.969112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.955477</td>\n",
       "      <td>0.918873</td>\n",
       "      <td>0.995119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  optimal_decision_threshold  optimal_f1_score  \\\n",
       "0       bogo-5-5-168                        0.51          0.941058   \n",
       "1     bogo-10-10-168                        0.54          0.951044   \n",
       "2       bogo-5-5-120                        0.54          0.940105   \n",
       "3  discount-5-20-240                        0.57          0.943446   \n",
       "4  discount-2-10-240                        0.57          0.964458   \n",
       "5     bogo-10-10-120                        0.61          0.935721   \n",
       "6  discount-2-10-168                        0.55          0.926541   \n",
       "7   discount-3-7-168                        0.57          0.955477   \n",
       "\n",
       "   optimal_precision  optimal_recall  \n",
       "0           0.896046        0.990832  \n",
       "1           0.917949        0.986614  \n",
       "2           0.901882        0.981712  \n",
       "3           0.906487        0.983547  \n",
       "4           0.937017        0.993556  \n",
       "5           0.931444        0.940037  \n",
       "6           0.887553        0.969112  \n",
       "7           0.918873        0.995119  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/diagnostic_metrics/test_optimal_f1_thresholds.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "* __Training Model Evaluation:__ After eight different random forest models were trained, I used the split training sets, X_train_v3 and y_train_v3, to generate cross-validation (with kfold = 6) diagnostic metrics. This process provides a more realistic picture of model performance before using the test set. Overall, the results look good.\n",
    "    * Precision and recall metrics are calculated using `sklearn.model_selection.cross_val_predict` function, and the results are looking solid across the board. \n",
    "    * F1 scores and ROC AUC's are all in the mid to high 90% range. \n",
    "    * If all customers are able to have the features generated during the feature engineering process, then we should be able to plug these customers' attributes into the models and generate reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_cv_score</th>\n",
       "      <th>accuracy_cv_stddev</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc_score (cross_val_score)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932251</td>\n",
       "      <td>0.009086</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.989255</td>\n",
       "      <td>0.942848</td>\n",
       "      <td>0.976598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950275</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.981395</td>\n",
       "      <td>0.948315</td>\n",
       "      <td>0.985364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918026</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.882226</td>\n",
       "      <td>0.985838</td>\n",
       "      <td>0.931158</td>\n",
       "      <td>0.968386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940359</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.972841</td>\n",
       "      <td>0.934160</td>\n",
       "      <td>0.982780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941017</td>\n",
       "      <td>0.011880</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>0.968877</td>\n",
       "      <td>0.934726</td>\n",
       "      <td>0.986862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929023</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>0.897180</td>\n",
       "      <td>0.977520</td>\n",
       "      <td>0.935628</td>\n",
       "      <td>0.975838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936785</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.917950</td>\n",
       "      <td>0.995393</td>\n",
       "      <td>0.955104</td>\n",
       "      <td>0.974016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0.950156</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.936311</td>\n",
       "      <td>0.995784</td>\n",
       "      <td>0.965132</td>\n",
       "      <td>0.977149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  accuracy  accuracy_cv_score  accuracy_cv_stddev  \\\n",
       "0       bogo-5-5-168  1.000000           0.932251            0.009086   \n",
       "1     bogo-10-10-168  1.000000           0.950275            0.007625   \n",
       "2       bogo-5-5-120  1.000000           0.918026            0.010330   \n",
       "3  discount-5-20-240  1.000000           0.940359            0.007936   \n",
       "4     bogo-10-10-120  1.000000           0.941017            0.011880   \n",
       "5  discount-2-10-168  1.000000           0.929023            0.011651   \n",
       "6   discount-3-7-168  1.000000           0.936785            0.008056   \n",
       "7  discount-2-10-240  0.999805           0.950156            0.004508   \n",
       "\n",
       "   precision_score  recall_score  f1_score  roc_auc_score (cross_val_score)  \n",
       "0         0.900600      0.989255  0.942848                         0.976598  \n",
       "1         0.917391      0.981395  0.948315                         0.985364  \n",
       "2         0.882226      0.985838  0.931158                         0.968386  \n",
       "3         0.898438      0.972841  0.934160                         0.982780  \n",
       "4         0.902900      0.968877  0.934726                         0.986862  \n",
       "5         0.897180      0.977520  0.935628                         0.975838  \n",
       "6         0.917950      0.995393  0.955104                         0.974016  \n",
       "7         0.936311      0.995784  0.965132                         0.977149  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/diagnostic_metrics/train_model_stats.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Test Set Evaluation:__ The test set produced solid numbers across the board. In examining the F1 scores and ROC AUC more closely, these numbers were indicating that the model can produce predictions with high precision and high recall. The ROC curves also indicate that the model performance across all eight models were very strong. \n",
    "    * The current F1 scores hold up very well against the optimized F1 scores from an earlier exercise. In other words, the decision threshold tuning is not necessary for this project.\n",
    "    * In terms of how sensitive the models are to the input values, it's good to have a general understanding of how this could impact the models going forward. Leveraging the work I did earlier with permutation importance, we can extrapolate a general framework to measure model sensitivity. In a nutshell, permutation feature importance measures the decrease in model performance when a single feature is randomly shuffled. The output is a list of features with their predictive contribution to the model. In another words, the shuffled feature allows the measurement of decrease in model score and thus quantifying the impact on model performance. \n",
    "    * The current models will start to degrade if features found in trained_models/trainging_feature_sets.json change over time. Therefore, monitoring the feature distribution over time will be helpful, including outlier monitoring if possible.\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/test_roc_curves.png\" style=\"width:600px;height:600px\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_bogo_offer_completed',\n",
       " 'total_reward_amount',\n",
       " 'offer_completion_rate',\n",
       " 'offer_received_time',\n",
       " 'avg_reward_per_oc_transaction',\n",
       " 'median_offer_duration',\n",
       " 'num_transactions_oc_direct',\n",
       " 'num_offer_completed',\n",
       " 'percent_oc_direct_transactions',\n",
       " 'transaction_aos_oc',\n",
       " 'num_oc_ch_social',\n",
       " 'avg_offer_completion_time',\n",
       " 'transaction_aos_no_oc',\n",
       " 'transaction_no_oc_amount',\n",
       " 'num_offer_completed_not_viewed',\n",
       " 'transaction_amount',\n",
       " 'num_bogo_offer_received',\n",
       " 'transaction_cnt']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "feature_sets = json.load(open('trained_models/training_feature_sets.json'))\n",
    "feature_sets['bogo-5-5-168']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Model Diagnostic Metrics for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>0.931518</td>\n",
       "      <td>0.896046</td>\n",
       "      <td>0.990832</td>\n",
       "      <td>0.941058</td>\n",
       "      <td>0.975653</td>\n",
       "      <td>989</td>\n",
       "      <td>163</td>\n",
       "      <td>13</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>0.947082</td>\n",
       "      <td>0.911466</td>\n",
       "      <td>0.988976</td>\n",
       "      <td>0.948640</td>\n",
       "      <td>0.979838</td>\n",
       "      <td>1178</td>\n",
       "      <td>122</td>\n",
       "      <td>14</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>0.927363</td>\n",
       "      <td>0.895156</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.938761</td>\n",
       "      <td>0.971952</td>\n",
       "      <td>898</td>\n",
       "      <td>158</td>\n",
       "      <td>18</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>0.945081</td>\n",
       "      <td>0.895609</td>\n",
       "      <td>0.988117</td>\n",
       "      <td>0.939591</td>\n",
       "      <td>0.985684</td>\n",
       "      <td>1311</td>\n",
       "      <td>126</td>\n",
       "      <td>13</td>\n",
       "      <td>1081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.932603</td>\n",
       "      <td>0.997071</td>\n",
       "      <td>0.963760</td>\n",
       "      <td>0.975934</td>\n",
       "      <td>666</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>0.939370</td>\n",
       "      <td>0.897611</td>\n",
       "      <td>0.970480</td>\n",
       "      <td>0.932624</td>\n",
       "      <td>0.987849</td>\n",
       "      <td>1303</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.877424</td>\n",
       "      <td>0.978378</td>\n",
       "      <td>0.925155</td>\n",
       "      <td>0.975082</td>\n",
       "      <td>1111</td>\n",
       "      <td>177</td>\n",
       "      <td>28</td>\n",
       "      <td>1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>0.938370</td>\n",
       "      <td>0.914525</td>\n",
       "      <td>0.998780</td>\n",
       "      <td>0.954797</td>\n",
       "      <td>0.974480</td>\n",
       "      <td>723</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>1637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  accuracy  precision    recall        f1   roc_auc    tn  \\\n",
       "0       bogo-5-5-168  0.931518   0.896046  0.990832  0.941058  0.975653   989   \n",
       "1     bogo-10-10-168  0.947082   0.911466  0.988976  0.948640  0.979838  1178   \n",
       "2       bogo-5-5-120  0.927363   0.895156  0.986832  0.938761  0.971952   898   \n",
       "3  discount-5-20-240  0.945081   0.895609  0.988117  0.939591  0.985684  1311   \n",
       "4  discount-2-10-240  0.948718   0.932603  0.997071  0.963760  0.975934   666   \n",
       "5     bogo-10-10-120  0.939370   0.897611  0.970480  0.932624  0.987849  1303   \n",
       "6  discount-2-10-168  0.920635   0.877424  0.978378  0.925155  0.975082  1111   \n",
       "7   discount-3-7-168  0.938370   0.914525  0.998780  0.954797  0.974480   723   \n",
       "\n",
       "    fp  fn    tp  \n",
       "0  163  13  1405  \n",
       "1  122  14  1256  \n",
       "2  158  18  1349  \n",
       "3  126  13  1081  \n",
       "4  123   5  1702  \n",
       "5  120  32  1052  \n",
       "6  177  28  1267  \n",
       "7  153   2  1637  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/diagnostic_metrics/test_model_stats.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Optimized Decision Thresholds using Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>optimal_decision_threshold</th>\n",
       "      <th>optimal_f1_score</th>\n",
       "      <th>optimal_precision</th>\n",
       "      <th>optimal_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.941058</td>\n",
       "      <td>0.896046</td>\n",
       "      <td>0.990832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.951044</td>\n",
       "      <td>0.917949</td>\n",
       "      <td>0.986614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.940105</td>\n",
       "      <td>0.901882</td>\n",
       "      <td>0.981712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.943446</td>\n",
       "      <td>0.906487</td>\n",
       "      <td>0.983547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.964458</td>\n",
       "      <td>0.937017</td>\n",
       "      <td>0.993556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.935721</td>\n",
       "      <td>0.931444</td>\n",
       "      <td>0.940037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.926541</td>\n",
       "      <td>0.887553</td>\n",
       "      <td>0.969112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.955477</td>\n",
       "      <td>0.918873</td>\n",
       "      <td>0.995119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  optimal_decision_threshold  optimal_f1_score  \\\n",
       "0       bogo-5-5-168                        0.51          0.941058   \n",
       "1     bogo-10-10-168                        0.54          0.951044   \n",
       "2       bogo-5-5-120                        0.54          0.940105   \n",
       "3  discount-5-20-240                        0.57          0.943446   \n",
       "4  discount-2-10-240                        0.57          0.964458   \n",
       "5     bogo-10-10-120                        0.61          0.935721   \n",
       "6  discount-2-10-168                        0.55          0.926541   \n",
       "7   discount-3-7-168                        0.57          0.955477   \n",
       "\n",
       "   optimal_precision  optimal_recall  \n",
       "0           0.896046        0.990832  \n",
       "1           0.917949        0.986614  \n",
       "2           0.901882        0.981712  \n",
       "3           0.906487        0.983547  \n",
       "4           0.937017        0.993556  \n",
       "5           0.931444        0.940037  \n",
       "6           0.887553        0.969112  \n",
       "7           0.918873        0.995119  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/diagnostic_metrics/test_optimal_f1_thresholds.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that the models significantly improved performance compared to their corresponding benchmark/baseline metrics. Before the important features were identified and the random forest classifiers were trained, the baseline metrics show that it was almost a fair coin toss and close to random. In the context of the ROC curve, the baseline ROC AUC is represented as the straight diagonal line, which represents the no-skill model. In comparison, all the trained models' ROC AUC scores are close to 1.0. The trained models are all predicting at 90%+ across all leading metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_type</th>\n",
       "      <th>baseline_precision</th>\n",
       "      <th>baseline_recall</th>\n",
       "      <th>baseline_f1_score</th>\n",
       "      <th>baseline_roc_auc</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bogo-5-5-168</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bogo-10-10-168</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bogo-5-5-120</td>\n",
       "      <td>0.5642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>discount-5-20-240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount-2-10-240</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bogo-10-10-120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>discount-2-10-168</td>\n",
       "      <td>0.5014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6679</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>discount-3-7-168</td>\n",
       "      <td>0.6517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7891</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer_type  baseline_precision  baseline_recall  baseline_f1_score  \\\n",
       "0       bogo-5-5-168              0.5518              1.0             0.7112   \n",
       "1     bogo-10-10-168              0.0000              0.0             0.0000   \n",
       "2       bogo-5-5-120              0.5642              1.0             0.7214   \n",
       "3  discount-5-20-240              0.0000              0.0             0.0000   \n",
       "4  discount-2-10-240              0.6839              1.0             0.8123   \n",
       "5     bogo-10-10-120              0.0000              0.0             0.0000   \n",
       "6  discount-2-10-168              0.5014              1.0             0.6679   \n",
       "7   discount-3-7-168              0.6517              1.0             0.7891   \n",
       "\n",
       "   baseline_roc_auc  baseline_accuracy  \n",
       "0               0.5             0.5518  \n",
       "1               0.5             0.5058  \n",
       "2               0.5             0.5642  \n",
       "3               0.5             0.5678  \n",
       "4               0.5             0.6839  \n",
       "5               0.5             0.5676  \n",
       "6               0.5             0.5014  \n",
       "7               0.5             0.6517  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/diagnostic_metrics/baseline_metrics.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "* This pipeline design, as shown below, was initially what I proposed, but during the build process I soon realized things had to change. The linear regression model that I had initially try to build produced bad results. At the same time, my initial approach of creating cohorts was much harder than expected. As a result, the whole design was revamped. It made more sense to make offer conversion predictions and targeting customers with specific offers.\n",
    "\n",
    "<img src=\"output/diagnostic_metrics/starbucks_proposal_pipeline_design.png\" style=\"width:650px;height:450px\"/>\n",
    "\n",
    "### Reflection\n",
    "* First, data marts were created to make analysis and aggregations faster. Next, feature engineering layers were created to build, transform, and engineer features. Feature selection layer was built to remove correlated features and retain important ones through random forest feature importance and permutation importance. Random forest classifiers were trained and pickled for each offer type. The test sets were used to measure model performance, and test predictions were serialized and saved. The recommendation logic, which takes the maximum prediction probability, selected the winning offer type. For customers who received only negative predictions, they received a randomly selected offer type.\n",
    "* One of the challenges was building the attribution logic to attribute offer events to transactions. I took on the challenge to build this out because, in my mind, having this aggregated view would expedite every part of this project.\n",
    "* Another challenge was thinking through how to approach the problem. The process of developing a framework to tackle the problem took time. \n",
    "* The feature engineering step was time consuming and coming up with features also took a long time. This stage required critical and creative thinking. At the end of the day, you want good features with good predictive power. \n",
    "* Feature selection process required a lot of thinking as well. There is no standard process for this, but I came up with an approach that was well suited for this project. \n",
    "\n",
    "### Improvement\n",
    "* The script run times could be improved by parallelizing the execution process with Dask.\n",
    "* The full pipeline could be more standardized and redundant code could be removed as well.\n",
    "* Other feature selection techniques could be leveraged to improve feature selection process.\n",
    "* The attribution logic used in the transaction_engagement script could be improved to capture edge cases.\n",
    "* Other model algorithms could be explored to improve overall model performance.\n",
    "* Number of model estimators could be reduced by grouping similar offer types together.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
